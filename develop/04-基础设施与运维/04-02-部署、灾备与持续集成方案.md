# 部署、灾备与持续集成方案

## 1. 部署架构设计

### 1.1 多环境部署策略

本平台采用**四环境五阶段**部署模型，确保从开发到生产的全流程可控，满足高频交易系统对稳定性与快速迭代的双重需求。

```mermaid
graph TB
    %% 环境层次
    subgraph ENVIRONMENTS[四层部署环境]
        DEV[开发环境<br/>DEV]
        TEST[测试环境<br/>TEST]
        STAGING[预发环境<br/>STAGING]
        PROD[生产环境<br/>PROD]
        
        DEV --> TEST
        TEST --> STAGING
        STAGING --> PROD
    end
    
    %% 部署流水线
    subgraph PIPELINE[五阶段部署流水线]
        BUILD[构建阶段]
        UNIT[单元测试]
        INTEGRATION[集成测试]
        PERFORMANCE[性能测试]
        DEPLOY[部署发布]
        
        BUILD --> UNIT
        UNIT --> INTEGRATION
        INTEGRATION --> PERFORMANCE
        PERFORMANCE --> DEPLOY
    end
    
    %% 环境与流水线关联
    BUILD -- 代码提交触发 --> DEV
    DEPLOY -- 自动/手动审批 --> TEST
    DEPLOY -- 生产数据副本 --> STAGING
    DEPLOY -- 蓝绿/金丝雀发布 --> PROD
    
    %% 部署特性
    subgraph FEATURES[关键部署特性]
        F1[基础设施即代码]
        F2[不可变基础设施]
        F3[蓝绿部署]
        F4[金丝雀发布]
        F5[自动回滚]
        F6[零停机部署]
        
        PROD --> F3
        PROD --> F4
        PROD --> F5
        PROD --> F6
    end
```

### 1.2 部署性能指标

| 部署环节 | 目标时间 | 成功率要求 | 回滚时间 | 关键影响 |
|---------|---------|-----------|----------|----------|
| 代码构建 | < 3分钟 | 99.9% | N/A | 开发迭代速度 |
| 单元测试 | < 5分钟 | 100% | N/A | 代码质量保证 |
| 集成测试 | < 15分钟 | 99.5% | N/A | 系统集成验证 |
| 性能测试 | < 30分钟 | 99% | N/A | 生产性能保障 |
| 预发部署 | < 10分钟 | 99.9% | < 5分钟 | 生产准备验证 |
| 生产部署 | < 15分钟 | 99.99% | < 2分钟 | 业务连续性 |
| 全流程 | < 1小时 | 99% | < 2分钟 | 整体交付效率 |

## 2. 容器化部署方案

### 2.1 Kubernetes部署架构

#### 2.1.1 集群架构设计

```yaml
# kubernetes/cluster-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: trading-cluster-config
  namespace: kube-system
data:
  # 集群基本信息
  cluster-name: "trading-production"
  cluster-region: "cn-east-1"
  cluster-environment: "production"
  
  # 网络配置
  network-plugin: "calico"
  service-cidr: "10.96.0.0/12"
  pod-cidr: "10.244.0.0/16"
  network-policy: "enabled"
  
  # DNS配置
  dns-service: "coredns"
  dns-domain: "trading.local"
  ndots: "2"
  
  # 存储配置
  storage-class: "fast-ssd"
  default-storage-size: "100Gi"
  backup-storage-class: "standard-hdd"
  
  # 监控配置
  monitoring-enabled: "true"
  logging-enabled: "true"
  tracing-enabled: "true"
  
---
# 节点池配置
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: trading-production
  region: cn-east-1
  version: "1.27"
  
nodeGroups:
  # 交易核心节点池（低延迟优化）
  - name: trading-core
    instanceType: c7i.8xlarge  # 32核64GB, 高频交易优化
    desiredCapacity: 4
    minSize: 4
    maxSize: 8
    volumeSize: 200
    volumeType: gp3
    iops: 16000
    throughput: 1000
    labels:
      node-type: "trading-core"
      workload: "latency-sensitive"
    taints:
      - key: "trading-core"
        value: "true"
        effect: "NoSchedule"
    kubeletExtraConfig:
      cpuManagerPolicy: "static"
      topologyManagerPolicy: "best-effort"
      reservedSystemCPUs: "0-3"
      systemReserved:
        cpu: "2"
        memory: "4Gi"
      kubeReserved:
        cpu: "1"
        memory: "2Gi"
    
  # 计算节点池（批处理任务）
  - name: compute-nodes
    instanceType: m6i.4xlarge  # 16核64GB
    desiredCapacity: 2
    minSize: 2
    maxSize: 4
    labels:
      node-type: "compute"
      workload: "batch-processing"
    
  # 服务节点池（Web/API服务）
  - name: service-nodes
    instanceType: t3.large  # 2核8GB
    desiredCapacity: 2
    minSize: 2
    maxSize: 4
    labels:
      node-type: "service"
      workload: "web-service"
    
  # 基础服务节点池（监控、日志等）
  - name: infrastructure-nodes
    instanceType: t3.xlarge  # 4核16GB
    desiredCapacity: 2
    minSize: 2
    maxSize: 3
    labels:
      node-type: "infrastructure"
      workload: "monitoring"
    taints:
      - key: "infrastructure"
        value: "true"
        effect: "NoSchedule"

# 网络策略配置
apiVersion: crd.projectcalico.org/v1
kind: GlobalNetworkPolicy
metadata:
  name: trading-network-policy
spec:
  # 默认拒绝所有流量
  types:
    - Ingress
    - Egress
  order: 1000
  selector: all()
  
---
# 命名空间隔离策略
apiVersion: v1
kind: Namespace
metadata:
  name: trading-production
  labels:
    name: "trading-production"
    environment: "production"
    security-tier: "high"
  annotations:
    scheduler.alpha.kubernetes.io/node-selector: "node-type=trading-core"

---
# 资源配额限制
apiVersion: v1
kind: ResourceQuota
metadata:
  name: trading-quota
  namespace: trading-production
spec:
  hard:
    requests.cpu: "100"
    requests.memory: "400Gi"
    limits.cpu: "200"
    limits.memory: "800Gi"
    pods: "200"
    services: "50"
    configmaps: "100"
    persistentvolumeclaims: "30"
    secrets: "100"
```

#### 2.1.2 交易应用部署配置

```yaml
# kubernetes/trading-engine-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trading-engine
  namespace: trading-production
  labels:
    app: trading-engine
    component: trading-core
    version: v1.2.0
  annotations:
    deployment.kubernetes.io/revision: "1"
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
spec:
  replicas: 4
  revisionHistoryLimit: 3
  minReadySeconds: 30
  progressDeadlineSeconds: 600
  
  selector:
    matchLabels:
      app: trading-engine
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # 确保零停机
  
  template:
    metadata:
      labels:
        app: trading-engine
        component: trading-core
        version: v1.2.0
      annotations:
        sidecar.istio.io/inject: "false"  # 禁用Service Mesh注入（低延迟要求）
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
    
    spec:
      # 节点选择与亲和性
      nodeSelector:
        node-type: "trading-core"
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: "In"
                    values:
                      - "trading-engine"
              topologyKey: "kubernetes.io/hostname"
        
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: "In"
                      values:
                        - "market-data"
                topologyKey: "kubernetes.io/hostname"
      
      # 容忍度
      tolerations:
        - key: "trading-core"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      
      # 优先级
      priorityClassName: "system-cluster-critical"
      
      # 安全上下文
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: "RuntimeDefault"
      
      containers:
        - name: trading-engine
          image: registry.trading.com/trading-engine:v1.2.0
          imagePullPolicy: IfNotPresent
          
          # 资源请求与限制
          resources:
            requests:
              cpu: "8"
              memory: "16Gi"
              hugepages-2Mi: "1Gi"
            limits:
              cpu: "16"
              memory: "32Gi"
              hugepages-2Mi: "2Gi"
          
          # 端口配置
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc-trading
              containerPort: 50051
              protocol: TCP
          
          # 环境变量
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: KAFKA_BROKERS
              value: "kafka-0.kafka.trading-production.svc.cluster.local:9092,kafka-1.kafka.trading-production.svc.cluster.local:9092"
            - name: REDIS_HOST
              value: "redis-master.redis.svc.cluster.local"
            - name: CTP_FRONT_ADDRESS
              valueFrom:
                configMapKeyRef:
                  name: ctp-config
                  key: front-address
          
          # 健康检查
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
              httpHeaders:
                - name: X-Custom-Header
                  value: "trading-engine"
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3
          
          startupProbe:
            httpGet:
              path: /startup
              port: 8080
            failureThreshold: 30
            periodSeconds: 10
          
          # 生命周期钩子
          lifecycle:
            postStart:
              exec:
                command:
                  - "/bin/sh"
                  - "-c"
                  - |
                    echo "Container started at $(date)" >> /var/log/startup.log
                    # 预热缓存
                    curl -s http://localhost:8080/warmup > /dev/null 2>&1
            preStop:
              exec:
                command:
                  - "/bin/sh"
                  - "-c"
                  - |
                    echo "Container stopping at $(date)" >> /var/log/shutdown.log
                    # 优雅关闭：停止接收新订单，完成处理中的订单
                    curl -s -X POST http://localhost:8080/graceful-shutdown > /dev/null 2>&1
                    sleep 30  # 等待30秒确保订单处理完成
          
          # 安全上下文（容器级别）
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - "ALL"
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: "RuntimeDefault"
          
          # 卷挂载
          volumeMounts:
            - name: config-volume
              mountPath: /etc/trading-engine
              readOnly: true
            - name: logs-volume
              mountPath: /var/log/trading-engine
            - name: hugepage
              mountPath: /hugepages
            - name: shared-memory
              mountPath: /dev/shm
      
      # 初始化容器
      initContainers:
        - name: init-config
          image: busybox:1.35
          command: ['sh', '-c', 'echo "Initializing trading engine configuration..." && sleep 2']
          volumeMounts:
            - name: config-volume
              mountPath: /etc/trading-engine
      
      # 卷定义
      volumes:
        - name: config-volume
          configMap:
            name: trading-engine-config
            items:
              - key: "application.yaml"
                path: "application.yaml"
              - key: "logging.yaml"
                path: "logging.yaml"
        
        - name: logs-volume
          emptyDir: {}
        
        - name: hugepage
          emptyDir:
            medium: HugePages
        
        - name: shared-memory
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi

---
# 服务配置
apiVersion: v1
kind: Service
metadata:
  name: trading-engine
  namespace: trading-production
  labels:
    app: trading-engine
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  selector:
    app: trading-engine
  
  ports:
    - name: http-metrics
      port: 8080
      targetPort: 8080
      protocol: TCP
    - name: grpc-trading
      port: 50051
      targetPort: 50051
      protocol: TCP
  
  type: LoadBalancer
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3小时

---
# 水平Pod自动缩放配置
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trading-engine-hpa
  namespace: trading-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trading-engine
  
  minReplicas: 4
  maxReplicas: 8
  
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: orders_per_second
        target:
          type: AverageValue
          averageValue: 5000
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 缩容稳定窗口5分钟
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
        - type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60  # 扩容稳定窗口1分钟
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 2
          periodSeconds: 30
      selectPolicy: Max
```

### 2.2 配置管理与密钥管理

#### 2.2.1 GitOps配置管理

```yaml
# argocd/trading-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: trading-platform
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: trading
  
  # 源配置：Git仓库
  source:
    repoURL: https://git.trading.com/trading-platform.git
    targetRevision: main
    path: kubernetes/production
    
    # 插件配置（如果需要模板渲染）
    plugin:
      name: kustomize
      env:
        - name: KUSTOMIZE_VERSION
          value: v4.5.7
  
  # 目标集群
  destination:
    server: https://kubernetes.default.svc
    namespace: trading-production
  
  # 同步策略
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  # 健康检查
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: trading-engine
      namespace: trading-production
    - apiVersion: apps/v1
      kind: Deployment
      name: risk-engine
      namespace: trading-production
    - apiVersion: v1
      kind: Service
      name: trading-engine
      namespace: trading-production
  
  # 忽略差异（某些字段由控制器管理）
  ignoreDifferences:
    - group: apps
      kind: Deployment
      name: trading-engine
      jsonPointers:
        - /spec/replicas
        - /spec/template/spec/containers/0/image
    - group: autoscaling
      kind: HorizontalPodAutoscaler
      name: trading-engine-hpa
      jsonPointers:
        - /spec/metrics

---
# Kustomize配置
# kubernetes/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: trading-production

# 基础配置
resources:
  - ../base
  
# 环境特定配置
patchesStrategicMerge:
  - deployment-patch.yaml
  - service-patch.yaml
  - configmap-patch.yaml

# 镜像替换
images:
  - name: trading-engine
    newName: registry.trading.com/trading-engine
    newTag: v1.2.0
  
  - name: risk-engine
    newName: registry.trading.com/risk-engine
    newTag: v1.1.0
  
  - name: market-data
    newName: registry.trading.com/market-data
    newTag: v1.0.0

# 配置生成
configMapGenerator:
  - name: trading-config
    files:
      - config/application.yaml
      - config/logging.yaml
    options:
      labels:
        app: trading-engine
        environment: production

secretGenerator:
  - name: trading-secrets
    files:
      - secrets/database-password
      - secrets/redis-password
      - secrets/kafka-password
    type: Opaque
    options:
      labels:
        app: trading-engine
        environment: production

# 变量替换
vars:
  - name: REPLICA_COUNT
    objref:
      kind: Deployment
      name: trading-engine
      apiVersion: apps/v1
    fieldref:
      fieldpath: spec.replicas
```

#### 2.2.2 密钥管理方案

```yaml
# vault/trading-secrets.yaml
apiVersion: external-secrets.io/v1beta1
kind: ClusterSecretStore
metadata:
  name: vault-backend
spec:
  provider:
    vault:
      server: "https://vault.trading.com:8200"
      path: "kubernetes"
      version: "v2"
      
      # 认证方式：Kubernetes Service Account
      auth:
        kubernetes:
          mountPath: "kubernetes"
          role: "trading-platform"
          serviceAccountRef:
            name: external-secrets
            namespace: external-secrets
      
      # TLS配置
      caBundle: |
        -----BEGIN CERTIFICATE-----
        # Vault CA证书
        -----END CERTIFICATE-----

---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: trading-secrets
  namespace: trading-production
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: ClusterSecretStore
  
  target:
    name: trading-secrets
    creationPolicy: Owner
    template:
      type: Opaque
      metadata:
        labels:
          app: trading-engine
          managed-by: external-secrets
        annotations:
          description: "Trading platform secrets from Vault"
  
  data:
    - secretKey: CTP_PASSWORD
      remoteRef:
        key: trading/data/production/ctp
        property: password
    
    - secretKey: DATABASE_PASSWORD
      remoteRef:
        key: trading/data/production/database
        property: password
    
    - secretKey: REDIS_PASSWORD
      remoteRef:
        key: trading/data/production/redis
        property: password
    
    - secretKey: KAFKA_SASL_PASSWORD
      remoteRef:
        key: trading/data/production/kafka
        property: sasl_password
    
    - secretKey: TLS_PRIVATE_KEY
      remoteRef:
        key: trading/data/production/tls
        property: private_key
      template:
        type: kubernetes.io/tls

  dataFrom:
    - extract:
        key: trading/data/production/api-keys
    - extract:
        key: trading/data/production/oauth-tokens

---
# Vault策略配置
# vault/policies/trading-platform.hcl
path "trading/data/production/*" {
  capabilities = ["read", "list"]
}

path "trading/metadata/production/*" {
  capabilities = ["read", "list"]
}

path "trading/data/staging/*" {
  capabilities = ["read", "list"]
}

path "sys/leases/renew" {
  capabilities = ["update"]
}

path "sys/leases/revoke" {
  capabilities = ["update"]
}

---
# Kubernetes认证配置
# vault/auth/kubernetes/trading-platform.hcl
path "auth/kubernetes/login" {
  capabilities = ["create", "read"]
}

path "auth/kubernetes/role/trading-platform" {
  capabilities = ["read"]
}

---
# Vault角色配置
# vault/roles/trading-platform.hcl
{
  "bound_service_account_names": ["trading-engine", "risk-engine", "market-data"],
  "bound_service_account_namespaces": ["trading-production", "trading-staging"],
  "policies": ["trading-platform"],
  "ttl": "1h",
  "max_ttl": "24h",
  "period": "24h"
}
```

## 3. 灾备与高可用设计

### 3.1 多活数据中心架构

```mermaid
graph TB
    %% 主数据中心
    subgraph DC1[主数据中心 - 上海]
        AZ1_A[可用区A]
        AZ1_B[可用区B]
        
        subgraph AZ1_A
            K8S_MASTER_A[K8S Master]
            ETCD_A[ETCD集群]
            API_A[API网关]
        end
        
        subgraph AZ1_B
            K8S_WORKER_B[交易节点]
            DB_MASTER_B[数据库主]
            CACHE_B[缓存集群]
        end
        
        AZ1_A <-->|低延迟专线| AZ1_B
    end
    
    %% 备用数据中心
    subgraph DC2[备用数据中心 - 北京]
        AZ2_A[可用区A]
        AZ2_B[可用区B]
        
        subgraph AZ2_A
            K8S_WORKER_A[交易节点]
            DB_SLAVE_A[数据库从]
            CACHE_A[缓存集群]
        end
        
        subgraph AZ2_B
            K8S_MASTER_B[K8S Master备份]
            ETCD_B[ETCD备份]
            API_B[API网关备份]
        end
        
        AZ2_A <-->|低延迟专线| AZ2_B
    end
    
    %% 数据同步
    DC1 -->|异步数据复制<br/>RPO < 5分钟| DC2
    DC1 -->|实时交易同步<br/>RPO < 1秒| DC2
    
    %% 全局负载均衡
    GSLB[全局负载均衡器] --> DC1
    GSLB --> DC2
    
    %% 客户端
    CLIENTS[交易客户端] --> GSLB
    
    %% 监控与切换
    MONITOR[灾备监控中心] -.->|健康检查| DC1
    MONITOR -.->|健康检查| DC2
    MONITOR -->|自动切换| GSLB
    
    %% 网络连接
    DC1 <-->|高速专线<br/>延迟 < 10ms| DC2
```

### 3.2 数据备份与恢复策略

#### 3.2.1 数据库备份方案

```yaml
# backup/database-backup.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-backup-config
  namespace: trading-production
data:
  # 备份策略配置
  backup-strategy: |
    {
      "full_backup": {
        "schedule": "0 2 * * *",  # 每天凌晨2点
        "retention_days": 30,
        "storage_class": "standard",
        "compression": "zstd"
      },
      "incremental_backup": {
        "schedule": "*/30 * * * *",  # 每30分钟
        "retention_hours": 24,
        "storage_class": "fast",
        "compression": "lz4"
      },
      "transaction_log_backup": {
        "schedule": "*/5 * * * *",  # 每5分钟
        "retention_hours": 6,
        "storage_class": "ultrafast",
        "compression": "none"
      }
    }
  
  # 恢复策略配置
  recovery-strategy: |
    {
      "rto_targets": {
        "critical": "5m",
        "high": "15m",
        "medium": "1h",
        "low": "4h"
      },
      "rpo_targets": {
        "critical": "1s",
        "high": "5s",
        "medium": "1m",
        "low": "5m"
      },
      "recovery_procedures": {
        "full_disaster": "switch_to_dr_site",
        "partial_corruption": "point_in_time_recovery",
        "accidental_deletion": "table_level_restore"
      }
    }

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-full-backup
  namespace: trading-production
spec:
  schedule: "0 2 * * *"
  startingDeadlineSeconds: 300
  concurrencyPolicy: Forbid
  suspend: false
  jobTemplate:
    spec:
      backoffLimit: 3
      template:
        spec:
          serviceAccountName: backup-service-account
          restartPolicy: OnFailure
          
          containers:
            - name: backup-agent
              image: percona/percona-xtrabackup:8.0
              imagePullPolicy: IfNotPresent
              
              env:
                - name: BACKUP_TYPE
                  value: "full"
                - name: DATABASE_HOST
                  value: "postgres-master.trading-production.svc.cluster.local"
                - name: DATABASE_PORT
                  value: "5432"
                - name: BACKUP_STORAGE
                  value: "s3://trading-backups/database/full"
                - name: AWS_REGION
                  value: "cn-north-1"
                - name: ENCRYPTION_KEY
                  valueFrom:
                    secretKeyRef:
                      name: backup-encryption-key
                      key: encryption-key
              
              resources:
                requests:
                  cpu: "2"
                  memory: "4Gi"
                limits:
                  cpu: "4"
                  memory: "8Gi"
              
              volumeMounts:
                - name: backup-tmp
                  mountPath: /backup-tmp
                - name: ssl-certs
                  mountPath: /etc/ssl/certs
                  readOnly: true
              
              command:
                - "/bin/bash"
                - "-c"
                - |
                  set -e
                  
                  # 创建备份时间戳
                  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                  
                  # 执行完整备份
                  echo "Starting full backup at $(date)"
                  xtrabackup \
                    --backup \
                    --host=${DATABASE_HOST} \
                    --port=${DATABASE_PORT} \
                    --user=${DATABASE_USER} \
                    --password=${DATABASE_PASSWORD} \
                    --target-dir=/backup-tmp/full_${TIMESTAMP} \
                    --compress=zstd \
                    --compress-threads=4 \
                    --encrypt=AES256 \
                    --encrypt-key=${ENCRYPTION_KEY} \
                    --encrypt-threads=4
                  
                  # 上传到S3
                  echo "Uploading backup to S3"
                  aws s3 sync /backup-tmp/full_${TIMESTAMP} ${BACKUP_STORAGE}/${TIMESTAMP}/
                  
                  # 清理临时文件
                  rm -rf /backup-tmp/full_${TIMESTAMP}
                  
                  # 记录备份完成
                  echo "Full backup completed at $(date)"
                  
                  # 发送通知
                  curl -X POST -H 'Content-Type: application/json' \
                    -d '{"type":"database_backup","status":"success","timestamp":"'${TIMESTAMP}'"}' \
                    https://monitoring.trading.com/api/backup-notification
          
          volumes:
            - name: backup-tmp
              emptyDir:
                sizeLimit: 100Gi
            - name: ssl-certs
              hostPath:
                path: /etc/ssl/certs
                type: Directory

---
# 数据库恢复作业
apiVersion: batch/v1
kind: Job
metadata:
  name: database-recovery-test
  namespace: trading-production
  annotations:
    test-frequency: "weekly"
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 1
  template:
    spec:
      serviceAccountName: recovery-service-account
      restartPolicy: Never
      
      containers:
        - name: recovery-agent
          image: percona/percona-xtrabackup:8.0
          imagePullPolicy: IfNotPresent
          
          env:
            - name: RECOVERY_TYPE
              value: "test"
            - name: BACKUP_TIMESTAMP
              value: "latest"
            - name: TEST_DATABASE_HOST
              value: "postgres-test.trading-production.svc.cluster.local"
            - name: BACKUP_STORAGE
              value: "s3://trading-backups/database/full"
            
          resources:
            requests:
              cpu: "4"
              memory: "8Gi"
            limits:
              cpu: "8"
              memory: "16Gi"
          
          command:
            - "/bin/bash"
            - "-c"
            - |
              set -e
              
              # 获取最新备份
              if [ "${BACKUP_TIMESTAMP}" = "latest" ]; then
                BACKUP_PATH=$(aws s3 ls ${BACKUP_STORAGE}/ | sort -r | head -1 | awk '{print $2}')
                BACKUP_PATH=${BACKUP_PATH%/}
              else
                BACKUP_PATH=${BACKUP_TIMESTAMP}
              fi
              
              echo "Recovering from backup: ${BACKUP_PATH}"
              
              # 下载备份
              aws s3 sync ${BACKUP_STORAGE}/${BACKUP_PATH}/ /recovery-data
              
              # 准备恢复
              xtrabackup --prepare --target-dir=/recovery-data
              
              # 停止测试数据库
              pg_ctl -D /var/lib/postgresql/data -m fast -w stop
              
              # 清理数据目录
              rm -rf /var/lib/postgresql/data/*
              
              # 恢复数据
              xtrabackup --copy-back --target-dir=/recovery-data
              
              # 启动数据库
              pg_ctl -D /var/lib/postgresql/data -l /var/log/postgresql/startup.log start
              
              # 验证数据
              psql -h ${TEST_DATABASE_HOST} -U postgres -c "SELECT COUNT(*) FROM orders;" || true
              
              echo "Recovery test completed successfully"
              
              # 发送测试报告
              curl -X POST -H 'Content-Type: application/json' \
                -d '{"type":"recovery_test","status":"success","backup_used":"'${BACKUP_PATH}'"}' \
                https://monitoring.trading.com/api/recovery-test
```

#### 3.2.2 交易数据实时同步

```python
# Python 交易数据实时同步服务
class TradingDataReplicator:
    """交易数据实时复制服务（跨数据中心）"""
    
    def __init__(self, source_cluster: str, target_cluster: str):
        self.source_cluster = source_cluster
        self.target_cluster = target_cluster
        self.replication_lag = 0  # 毫秒
        self.replication_status = "healthy"
        
        # 初始化连接
        self.source_conn = self._connect_to_cluster(source_cluster)
        self.target_conn = self._connect_to_cluster(target_cluster)
        
        # 启动监控线程
        self.monitor_thread = threading.Thread(target=self._monitor_replication, daemon=True)
        self.monitor_thread.start()
    
    def _connect_to_cluster(self, cluster: str) -> Dict:
        """连接到数据库集群"""
        if cluster == "primary":
            return {
                "host": "postgres-master.trading-production.svc.cluster.local",
                "port": 5432,
                "database": "trading",
                "user": "replicator",
                "password": os.getenv("REPLICATION_PASSWORD")
            }
        elif cluster == "dr":
            return {
                "host": "postgres-dr.trading-dr.svc.cluster.local",
                "port": 5432,
                "database": "trading",
                "user": "replicator",
                "password": os.getenv("DR_REPLICATION_PASSWORD")
            }
        else:
            raise ValueError(f"Unknown cluster: {cluster}")
    
    def start_replication(self):
        """启动数据复制"""
        LOG_INFO(f"Starting replication from {self.source_cluster} to {self.target_cluster}")
        
        try:
            # 1. 初始化复制槽
            self._create_replication_slot()
            
            # 2. 启动逻辑复制
            self._start_logical_replication()
            
            # 3. 启动变更数据捕获
            self._start_cdc_capture()
            
            self.replication_status = "active"
            LOG_INFO("Replication started successfully")
            
        except Exception as e:
            LOG_ERROR(f"Failed to start replication: {e}")
            self.replication_status = "failed"
            raise
    
    def _create_replication_slot(self):
        """创建复制槽"""
        conn = psycopg2.connect(**self.source_conn)
        conn.autocommit = True
        cursor = conn.cursor()
        
        try:
            # 检查是否已存在复制槽
            cursor.execute("""
                SELECT slot_name FROM pg_replication_slots 
                WHERE slot_name = 'trading_dr_replication'
            """)
            
            if cursor.fetchone() is None:
                # 创建逻辑复制槽
                cursor.execute("""
                    SELECT * FROM pg_create_logical_replication_slot(
                        'trading_dr_replication',
                        'pgoutput'
                    )
                """)
                LOG_INFO("Created replication slot: trading_dr_replication")
            else:
                LOG_INFO("Replication slot already exists")
        
        finally:
            cursor.close()
            conn.close()
    
    def _start_logical_replication(self):
        """启动逻辑复制"""
        # 使用WAL接收器流式复制
        source_conn = psycopg2.connect(**self.source_conn)
        source_conn.autocommit = True
        
        # 配置复制连接
        replication_conn = psycopg2.connect(
            host=self.source_conn["host"],
            port=self.source_conn["port"],
            user=self.source_conn["user"],
            password=self.source_conn["password"],
            database=self.source_conn["database"],
            replication="database"
        )
        
        # 启动复制
        cursor = replication_conn.cursor()
        cursor.execute(f"START_REPLICATION SLOT trading_dr_replication LOGICAL 0/0")
        
        # 处理复制流
        self._process_replication_stream(cursor)
    
    def _process_replication_stream(self, cursor):
        """处理复制数据流"""
        while self.replication_status == "active":
            try:
                # 接收WAL数据
                message = cursor.read_message()
                
                if message:
                    # 解析和处理消息
                    self._process_wal_message(message)
                    
                    # 发送反馈（避免WAL堆积）
                    cursor.send_feedback(write_lsn=message.data_start)
                
                # 更新复制延迟
                self._update_replication_lag()
                
            except Exception as e:
                LOG_ERROR(f"Error processing replication stream: {e}")
                self.replication_status = "error"
                break
    
    def _start_cdc_capture(self):
        """启动变更数据捕获"""
        # 捕获关键表的变更
        tables_to_capture = [
            "orders",
            "trades", 
            "positions",
            "accounts",
            "risk_limits"
        ]
        
        for table in tables_to_capture:
            thread = threading.Thread(
                target=self._capture_table_changes,
                args=(table,),
                daemon=True
            )
            thread.start()
    
    def _capture_table_changes(self, table_name: str):
        """捕获表级别变更"""
        conn = psycopg2.connect(**self.source_conn)
        conn.autocommit = True
        
        try:
            # 创建触发器捕获变更
            self._create_change_trigger(conn, table_name)
            
            # 轮询变更表
            while self.replication_status == "active":
                changes = self._get_pending_changes(conn, table_name)
                
                if changes:
                    # 复制变更到目标集群
                    self._apply_changes_to_target(changes, table_name)
                    
                    # 标记变更为已处理
                    self._mark_changes_processed(conn, changes)
                
                time.sleep(0.1)  # 100ms轮询间隔
        
        finally:
            conn.close()
    
    def _apply_changes_to_target(self, changes: List[Dict], table_name: str):
        """应用变更到目标集群"""
        target_conn = psycopg2.connect(**self.target_conn)
        target_conn.autocommit = False
        cursor = target_conn.cursor()
        
        try:
            for change in changes:
                operation = change["operation"]
                data = change["data"]
                
                if operation == "INSERT":
                    columns = ", ".join(data.keys())
                    placeholders = ", ".join(["%s"] * len(data))
                    values = list(data.values())
                    
                    query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
                    cursor.execute(query, values)
                
                elif operation == "UPDATE":
                    set_clause = ", ".join([f"{k} = %s" for k in data.keys()])
                    values = list(data.values())
                    values.append(change["id"])
                    
                    query = f"UPDATE {table_name} SET {set_clause} WHERE id = %s"
                    cursor.execute(query, values)
                
                elif operation == "DELETE":
                    query = f"DELETE FROM {table_name} WHERE id = %s"
                    cursor.execute(query, (change["id"],))
            
            target_conn.commit()
            
            # 更新统计
            self.replication_stats[table_name] = self.replication_stats.get(table_name, 0) + len(changes)
        
        except Exception as e:
            target_conn.rollback()
            LOG_ERROR(f"Failed to apply changes to {table_name}: {e}")
            raise
        
        finally:
            cursor.close()
            target_conn.close()
    
    def _monitor_replication(self):
        """监控复制状态"""
        while True:
            try:
                # 检查连接状态
                source_healthy = self._check_connection_health(self.source_conn)
                target_healthy = self._check_connection_health(self.target_conn)
                
                if not source_healthy or not target_healthy:
                    self.replication_status = "unhealthy"
                    LOG_WARN("Replication connection unhealthy")
                
                # 检查数据一致性
                consistency_check = self._check_data_consistency()
                if not consistency_check["consistent"]:
                    self.replication_status = "inconsistent"
                    LOG_ERROR(f"Data inconsistency detected: {consistency_check['details']}")
                
                # 发送监控指标
                self._send_monitoring_metrics()
                
                # 健康检查间隔
                time.sleep(5)  # 5秒检查间隔
            
            except Exception as e:
                LOG_ERROR(f"Error in replication monitor: {e}")
                time.sleep(10)
    
    def _check_data_consistency(self) -> Dict:
        """检查数据一致性"""
        try:
            # 抽样检查关键数据一致性
            checks = [
                self._compare_table_counts("orders"),
                self._compare_table_counts("trades"),
                self._compare_table_counts("positions"),
                self._compare_account_balances(),
                self._compare_risk_limits()
            ]
            
            inconsistent_checks = [c for c in checks if not c["consistent"]]
            
            return {
                "consistent": len(inconsistent_checks) == 0,
                "details": inconsistent_checks,
                "timestamp": time.time()
            }
        
        except Exception as e:
            LOG_ERROR(f"Error checking data consistency: {e}")
            return {
                "consistent": False,
                "details": [{"check": "consistency_check", "error": str(e)}],
                "timestamp": time.time()
            }
    
    def switch_to_dr(self) -> bool:
        """切换到灾备站点"""
        LOG_INFO("Initiating DR switchover")
        
        try:
            # 1. 停止主站点写入
            self._stop_primary_writes()
            
            # 2. 等待复制完成
            self._wait_for_replication_catchup()
            
            # 3. 提升灾备站点为主
            self._promote_dr_to_primary()
            
            # 4. 更新DNS/负载均衡
            self._update_traffic_routing()
            
            # 5. 通知监控系统
            self._notify_dr_switchover()
            
            LOG_INFO("DR switchover completed successfully")
            return True
        
        except Exception as e:
            LOG_ERROR(f"DR switchover failed: {e}")
            # 尝试回滚
            self._rollback_dr_switchover()
            return False
```

### 3.3 故障转移与恢复演练

```yaml
# dr/dr-drills.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-drill-config
  namespace: trading-production
data:
  # 故障转移演练计划
  drill-schedule: |
    {
      "weekly": {
        "type": "simulated_failure",
        "day": "saturday",
        "time": "02:00",
        "duration": "1h",
        "scope": "non-critical"
      },
      "monthly": {
        "type": "planned_failover",
        "day": "last_sunday",
        "time": "00:00", 
        "duration": "4h",
        "scope": "full_dr"
      },
      "quarterly": {
        "type": "unplanned_drill",
        "schedule": "random",
        "duration": "8h",
        "scope": "complete_simulation"
      }
    }
  
  # RTO/RPO目标
  recovery-objectives: |
    {
      "rto_targets": {
        "tier_1": "5m",    # 核心交易服务
        "tier_2": "15m",   # 风险监控服务
        "tier_3": "30m",   # 报表服务
        "tier_4": "2h"     # 分析服务
      },
      "rpo_targets": {
        "tier_1": "1s",    # 订单/成交数据
        "tier_2": "5s",    # 持仓数据
        "tier_3": "1m",    # 风险数据
        "tier_4": "5m"     # 历史数据
      }
    }

---
# 故障转移演练作业
apiVersion: batch/v1
kind: Job
metadata:
  name: dr-failover-drill
  namespace: trading-production
  annotations:
    drill-type: "monthly_planned"
    drill-id: "2024-01-drill-001"
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: dr-drill-service-account
      restartPolicy: Never
      
      initContainers:
        - name: drill-preparation
          image: busybox:1.35
          command:
            - "/bin/sh"
            - "-c"
            - |
              echo "Starting DR drill preparation"
              
              # 通知监控系统
              curl -X POST -H 'Content-Type: application/json' \
                -d '{"drill_id":"2024-01-drill-001","phase":"preparation","timestamp":"'$(date -Iseconds)'"}' \
                https://monitoring.trading.com/api/dr-drill
              
              # 创建演练快照
              echo "Creating pre-drill snapshot"
              
              # 等待准备完成
              sleep 10
              
              echo "Preparation completed"
      
      containers:
        - name: dr-orchestrator
          image: dr-orchestrator:1.0.0
          imagePullPolicy: IfNotPresent
          
          env:
            - name: DRILL_ID
              value: "2024-01-drill-001"
            - name: DRILL_TYPE
              value: "monthly_planned"
            - name: FAILURE_SCENARIO
              value: "datacenter_outage"
            - name: EXPECTED_RTO
              value: "5m"
            - name: EXPECTED_RPO
              value: "1s"
          
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
          
          command:
            - "/app/dr-orchestrator"
            - "start-drill"
          
          volumeMounts:
            - name: drill-config
              mountPath: /etc/dr-drill
            - name: drill-logs
              mountPath: /var/log/dr-drill
      
      volumes:
        - name: drill-config
          configMap:
            name: dr-drill-config
        - name: drill-logs
          emptyDir: {}

---
# 演练后验证
apiVersion: batch/v1
kind: Job
metadata:
  name: dr-drill-validation
  namespace: trading-production
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: dr-validation-service-account
      restartPolicy: Never
      
      containers:
        - name: validation-agent
          image: validation-agent:1.0.0
          imagePullPolicy: IfNotPresent
          
          env:
            - name: DRILL_ID
              value: "2024-01-drill-001"
          
          command:
            - "/bin/bash"
            - "-c"
            - |
              set -e
              
              echo "Starting DR drill validation"
              
              # 1. 验证服务可用性
              services_to_check=(
                "trading-engine.trading-production.svc.cluster.local:8080/health"
                "risk-engine.trading-production.svc.cluster.local:8080/health"
                "market-data.trading-production.svc.cluster.local:8080/health"
              )
              
              for service in "${services_to_check[@]}"; do
                echo "Checking service: $service"
                if curl -s --max-time 5 "http://$service" | grep -q '"status":"UP"'; then
                  echo "✓ Service $service is healthy"
                else
                  echo "✗ Service $service is not healthy"
                  exit 1
                fi
              done
              
              # 2. 验证数据一致性
              echo "Checking data consistency..."
              
              # 检查订单数据
              order_count_primary=$(psql -h postgres-master -U validator -t -c "SELECT COUNT(*) FROM orders;" 2>/dev/null || echo "0")
              order_count_dr=$(psql -h postgres-dr -U validator -t -c "SELECT COUNT(*) FROM orders;" 2>/dev/null || echo "0")
              
              if [ "$order_count_primary" -eq "$order_count_dr" ]; then
                echo "✓ Order counts match: $order_count_primary"
              else
                echo "✗ Order counts mismatch: primary=$order_count_primary, dr=$order_count_dr"
                exit 1
              fi
              
              # 3. 验证性能指标
              echo "Checking performance metrics..."
              
              # 获取当前延迟
              current_latency=$(curl -s http://monitoring.trading.com/api/metrics/order_latency_p99 | jq -r '.value')
              
              if (( $(echo "$current_latency < 10" | bc -l) )); then
                echo "✓ Latency within acceptable range: ${current_latency}ms"
              else
                echo "✗ Latency too high: ${current_latency}ms"
                exit 1
              fi
              
              # 4. 生成验证报告
              echo "Generating validation report..."
              
              report_data=$(cat <<EOF
              {
                "drill_id": "${DRILL_ID}",
                "validation_time": "$(date -Iseconds)",
                "services_checked": ${#services_to_check[@]},
                "services_healthy": ${#services_to_check[@]},
                "data_consistent": true,
                "performance_acceptable": true,
                "overall_result": "PASS",
                "details": {
                  "order_counts_match": true,
                  "latency_ms": ${current_latency}
                }
              }
              EOF
              )
              
              # 发送报告
              curl -X POST -H 'Content-Type: application/json' \
                -d "$report_data" \
                https://monitoring.trading.com/api/dr-drill-validation
              
              echo "DR drill validation completed successfully"
```

## 4. 持续集成与交付

### 4.1 CI/CD流水线设计

```yaml
# .github/workflows/trading-ci-cd.yaml
name: Trading Platform CI/CD

on:
  push:
    branches:
      - main
      - develop
      - 'release/**'
    paths:
      - 'src/**'
      - 'tests/**'
      - 'Dockerfile'
      - 'requirements.txt'
      - 'pyproject.toml'
      - 'Makefile'
  
  pull_request:
    branches:
      - main
      - develop
  
  # 手动触发
  workflow_dispatch:
    inputs:
      environment:
        description: '部署环境'
        required: true
        default: 'staging'
        type: choice
        options:
          - dev
          - test
          - staging
          - production
      version:
        description: '版本号'
        required: false
        type: string

# 环境变量
env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  DOCKER_BUILDKIT: 1
  
  # 版本号
  VERSION: ${{ github.sha }}
  SHORT_SHA: ${{ github.sha }}
  
  # 环境特定变量
  KUBE_NAMESPACE: ${{ github.event.inputs.environment || 'staging' }}

jobs:
  # 代码质量检查
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-22.04
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements-dev.txt
          pip install black==23.3.0 flake8==6.0.0 mypy==1.3.0 pylint==2.17.4
      
      - name: Code formatting (Black)
        run: black --check --diff src/ tests/
      
      - name: Linting (Flake8)
        run: flake8 src/ tests/ --max-line-length=100 --exclude=__pycache__
      
      - name: Type checking (MyPy)
        run: mypy src/ --ignore-missing-imports --strict
      
      - name: Code complexity (Pylint)
        run: pylint src/ --fail-under=9.0
      
      - name: Security scanning (Bandit)
        run: bandit -r src/ -ll

  # 单元测试
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-22.04
    needs: code-quality
    timeout-minutes: 15
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run unit tests
        run: |
          python -m pytest tests/unit/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term \
            --junitxml=junit.xml \
            -v
        env:
          PYTHONPATH: src/
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            junit.xml
            coverage.xml
            htmlcov/

  # 集成测试
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-22.04
    needs: unit-tests
    timeout-minutes: 30
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: trading_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      
      kafka:
        image: confluentinc/cp-kafka:7.3.2
        env:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: localhost:2181
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        ports:
          - 9092:9092
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Wait for services
        run: |
          # 等待PostgreSQL
          until pg_isready -h localhost -p 5432; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          
          # 等待Redis
          until redis-cli -h localhost ping | grep -q PONG; do
            echo "Waiting for Redis..."
            sleep 2
          done
          
          # 等待Kafka
          until kafka-topics --bootstrap-server localhost:9092 --list; do
            echo "Waiting for Kafka..."
            sleep 5
          done
      
      - name: Run integration tests
        run: |
          python -m pytest tests/integration/ \
            --cov=src \
            --cov-append \
            --cov-report=xml \
            --junitxml=junit-integration.xml \
            -v
        env:
          PYTHONPATH: src/
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/trading_test
          REDIS_URL: redis://localhost:6379/0
          KAFKA_BROKERS: localhost:9092
      
      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            junit-integration.xml
            coverage.xml

  # 性能测试
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-22.04
    needs: integration-tests
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-perf.txt
      
      - name: Run performance tests
        run: |
          python -m pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=benchmark.json \
            --benchmark-min-rounds=10 \
            -v
        env:
          PYTHONPATH: src/
      
      - name: Analyze performance results
        run: |
          python scripts/analyze_performance.py benchmark.json
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            benchmark.json
            performance_report.html

  # 构建Docker镜像
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-22.04
    needs: performance-tests
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Login to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=sha,prefix={{branch}}-
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,value=latest,enable={{is_default_branch}}
      
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64
      
      - name: Image vulnerability scanning
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
      
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # 部署到环境
  deploy:
    name: Deploy to ${{ github.event.inputs.environment || 'staging' }}
    runs-on: ubuntu-22.04
    needs: build-and-push
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    environment: ${{ github.event.inputs.environment || 'staging' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure Kubernetes
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
      
      - name: Setup Kubernetes context
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=$(pwd)/kubeconfig
          
          # 验证连接
          kubectl cluster-info
          kubectl get nodes
      
      - name: Deploy using Helm
        run: |
          export KUBECONFIG=$(pwd)/kubeconfig
          
          # 更新Helm仓库
          helm repo add trading https://charts.trading.com/
          helm repo update
          
          # 部署应用
          helm upgrade --install trading-platform trading/trading-platform \
            --namespace ${{ env.KUBE_NAMESPACE }} \
            --create-namespace \
            --values helm/values-${{ env.KUBE_NAMESPACE }}.yaml \
            --set image.tag=${{ env.VERSION }} \
            --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
            --wait \
            --timeout 10m \
            --atomic
      
      - name: Run post-deployment tests
        run: |
          export KUBECONFIG=$(pwd)/kubeconfig
          
          # 等待部署就绪
          kubectl wait --for=condition=available \
            --timeout=300s \
            deployment/trading-engine \
            -n ${{ env.KUBE_NAMESPACE }}
          
          # 运行冒烟测试
          python tests/smoke/deployment_smoke.py \
            --namespace ${{ env.KUBE_NAMESPACE }}
      
      - name: Notify deployment status
        if: always()
        run: |
          DEPLOYMENT_STATUS=${{ job.status }}
          
          curl -X POST -H 'Content-Type: application/json' \
            -d '{
              "environment": "${{ env.KUBE_NAMESPACE }}",
              "version": "${{ env.VERSION }}",
              "status": "$DEPLOYMENT_STATUS",
              "commit": "${{ github.sha }}",
              "timestamp": "$(date -Iseconds)"
            }' \
            https://monitoring.trading.com/api/deployment-notification

  # 生产环境部署（需要审批）
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-22.04
    needs: deploy
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production
    
    steps:
      - name: Wait for manual approval
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.token }}
          approvers: ${{ secrets.PRODUCTION_APPROVERS }}
          minimum-approvals: 2
          issue-title: "Production Deployment Approval"
          issue-body: |
            ### Production Deployment Request
            
            **Application**: Trading Platform
            **Version**: ${{ env.VERSION }}
            **Commit**: ${{ github.sha }}
            **Deployed to Staging**: ✅ Success
            
            **Changes included**:
            ```diff
            ${{ github.event.commits[0].message }}
            ```
            
            Please review and approve this deployment.
      
      - name: Deploy to Production
        run: |
          echo "Starting production deployment..."
          # 生产环境部署逻辑
          # （与staging部署类似，但包含额外的验证步骤）
```

### 4.2 部署验证与回滚

```python
# Python 部署验证与回滚工具
class DeploymentValidator:
    """部署验证与自动回滚工具"""
    
    def __init__(self, namespace: str, deployment_name: str):
        self.namespace = namespace
        self.deployment_name = deployment_name
        self.k8s_client = self._init_k8s_client()
        self.previous_revision = None
        self.health_checks = []
        
    def _init_k8s_client(self):
        """初始化Kubernetes客户端"""
        from kubernetes import client, config
        
        try:
            config.load_kube_config()
        except:
            config.load_incluster_config()
        
        return client.AppsV1Api()
    
    def record_previous_revision(self):
        """记录部署前的版本"""
        deployment = self.k8s_client.read_namespaced_deployment(
            name=self.deployment_name,
            namespace=self.namespace
        )
        
        self.previous_revision = {
            "image": deployment.spec.template.spec.containers[0].image,
            "replicas": deployment.spec.replicas,
            "revision": deployment.metadata.annotations.get(
                "deployment.kubernetes.io/revision", "0"
            )
        }
        
        LOG_INFO(f"Recorded previous revision: {self.previous_revision}")
    
    def add_health_check(self, check_func: Callable, name: str, 
                        timeout: int = 30, interval: int = 5):
        """添加健康检查"""
        self.health_checks.append({
            "name": name,
            "func": check_func,
            "timeout": timeout,
            "interval": interval,
            "passed": False
        })
    
    def validate_deployment(self) -> Tuple[bool, List[Dict]]:
        """验证部署是否成功"""
        LOG_INFO(f"Starting deployment validation for {self.deployment_name}")
        
        validation_results = []
        all_passed = True
        
        # 1. 检查部署状态
        deployment_status = self._check_deployment_status()
        validation_results.append(deployment_status)
        
        if not deployment_status["passed"]:
            all_passed = False
            LOG_ERROR(f"Deployment status check failed: {deployment_status['details']}")
            return False, validation_results
        
        # 2. 检查Pod状态
        pod_status = self._check_pod_status()
        validation_results.append(pod_status)
        
        if not pod_status["passed"]:
            all_passed = False
            LOG_ERROR(f"Pod status check failed: {pod_status['details']}")
        
        # 3. 运行自定义健康检查
        for check in self.health_checks:
            check_result = self._run_health_check(check)
            validation_results.append(check_result)
            
            if not check_result["passed"]:
                all_passed = False
                LOG_ERROR(f"Health check failed: {check['name']}")
        
        # 4. 检查服务端点
        service_status = self._check_service_endpoints()
        validation_results.append(service_status)
        
        if not service_status["passed"]:
            all_passed = False
            LOG_ERROR(f"Service endpoint check failed: {service_status['details']}")
        
        # 5. 检查性能指标
        performance_status = self._check_performance_metrics()
        validation_results.append(performance_status)
        
        if not performance_status["passed"]:
            all_passed = False
            LOG_WARN(f"Performance check warning: {performance_status['details']}")
        
        LOG_INFO(f"Deployment validation completed: {'PASSED' if all_passed else 'FAILED'}")
        return all_passed, validation_results
    
    def _check_deployment_status(self) -> Dict:
        """检查部署状态"""
        try:
            deployment = self.k8s_client.read_namespaced_deployment(
                name=self.deployment_name,
                namespace=self.namespace
            )
            
            conditions = deployment.status.conditions or []
            available_condition = next(
                (c for c in conditions if c.type == "Available"), 
                None
            )
            progressing_condition = next(
                (c for c in conditions if c.type == "Progressing"), 
                None
            )
            
            is_available = (
                available_condition and 
                available_condition.status == "True"
            )
            is_progressing = (
                progressing_condition and 
                progressing_condition.status == "True"
            )
            is_ready = (
                deployment.status.ready_replicas == deployment.spec.replicas and
                deployment.status.updated_replicas == deployment.spec.replicas
            )
            
            passed = is_available and is_progressing and is_ready
            
            return {
                "name": "deployment_status",
                "passed": passed,
                "details": {
                    "available": is_available,
                    "progressing": is_progressing,
                    "ready_replicas": deployment.status.ready_replicas,
                    "desired_replicas": deployment.spec.replicas,
                    "updated_replicas": deployment.status.updated_replicas
                },
                "timestamp": time.time()
            }
            
        except Exception as e:
            LOG_ERROR(f"Error checking deployment status: {e}")
            return {
                "name": "deployment_status",
                "passed": False,
                "details": {"error": str(e)},
                "timestamp": time.time()
            }
    
    def _check_pod_status(self) -> Dict:
        """检查Pod状态"""
        from kubernetes import client
        
        v1 = client.CoreV1Api()
        
        try:
            pods = v1.list_namespaced_pod(
                namespace=self.namespace,
                label_selector=f"app={self.deployment_name}"
            )
            
            pod_statuses = []
            all_pods_ready = True
            
            for pod in pods.items:
                pod_ready = all(
                    condition.status == "True" 
                    for condition in pod.status.conditions 
                    if condition.type == "Ready"
                )
                
                pod_status = {
                    "name": pod.metadata.name,
                    "phase": pod.status.phase,
                    "ready": pod_ready,
                    "restart_count": sum(
                        container.restart_count 
                        for container in pod.status.container_statuses or []
                    )
                }
                
                pod_statuses.append(pod_status)
                
                if not pod_ready or pod.status.phase != "Running":
                    all_pods_ready = False
            
            return {
                "name": "pod_status",
                "passed": all_pods_ready,
                "details": {
                    "total_pods": len(pods.items),
                    "ready_pods": len([p for p in pod_statuses if p["ready"]]),
                    "pod_details": pod_statuses
                },
                "timestamp": time.time()
            }
            
        except Exception as e:
            LOG_ERROR(f"Error checking pod status: {e}")
            return {
                "name": "pod_status",
                "passed": False,
                "details": {"error": str(e)},
                "timestamp": time.time()
            }
    
    def _run_health_check(self, check: Dict) -> Dict:
        """运行单个健康检查"""
        start_time = time.time()
        
        try:
            # 使用超时机制运行检查
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(check["func"])
                
                try:
                    result = future.result(timeout=check["timeout"])
                    passed = bool(result)
                    
                    return {
                        "name": check["name"],
                        "passed": passed,
                        "details": {"result": str(result)},
                        "duration": time.time() - start_time,
                        "timestamp": time.time()
                    }
                    
                except concurrent.futures.TimeoutError:
                    LOG_ERROR(f"Health check timeout: {check['name']}")
                    return {
                        "name": check["name"],
                        "passed": False,
                        "details": {"error": "timeout"},
                        "duration": time.time() - start_time,
                        "timestamp": time.time()
                    }
        
        except Exception as e:
            LOG_ERROR(f"Error running health check {check['name']}: {e}")
            return {
                "name": check["name"],
                "passed": False,
                "details": {"error": str(e)},
                "duration": time.time() - start_time,
                "timestamp": time.time()
            }
    
    def _check_service_endpoints(self) -> Dict:
        """检查服务端点"""
        try:
            from kubernetes import client
            
            v1 = client.CoreV1Api()
            
            # 获取服务
            services = v1.list_namespaced_service(
                namespace=self.namespace,
                label_selector=f"app={self.deployment_name}"
            )
            
            endpoint_results = []
            all_endpoints_healthy = True
            
            for service in services.items:
                for port in service.spec.ports:
                    endpoint_url = f"http://{service.metadata.name}.{self.namespace}.svc.cluster.local:{port.port}/health"
                    
                    try:
                        response = requests.get(
                            endpoint_url, 
                            timeout=5,
                            headers={"User-Agent": "deployment-validator"}
                        )
                        
                        healthy = response.status_code == 200
                        
                        endpoint_results.append({
                            "service": service.metadata.name,
                            "port": port.port,
                            "healthy": healthy,
                            "status_code": response.status_code,
                            "response_time": response.elapsed.total_seconds()
                        })
                        
                        if not healthy:
                            all_endpoints_healthy = False
                            
                    except Exception as e:
                        LOG_ERROR(f"Error checking endpoint {endpoint_url}: {e}")
                        endpoint_results.append({
                            "service": service.metadata.name,
                            "port": port.port,
                            "healthy": False,
                            "error": str(e)
                        })
                        all_endpoints_healthy = False
            
            return {
                "name": "service_endpoints",
                "passed": all_endpoints_healthy,
                "details": {"endpoints": endpoint_results},
                "timestamp": time.time()
            }
            
        except Exception as e:
            LOG_ERROR(f"Error checking service endpoints: {e}")
            return {
                "name": "service_endpoints",
                "passed": False,
                "details": {"error": str(e)},
                "timestamp": time.time()
            }
    
    def _check_performance_metrics(self) -> Dict:
        """检查性能指标"""
        try:
            # 查询Prometheus获取关键性能指标
            prometheus_url = "http://prometheus.trading-production.svc.cluster.local:9090"
            
            queries = {
                "latency_p99": f'histogram_quantile(0.99, rate(order_processing_latency_ns_bucket{{app="{self.deployment_name}"}}[5m]))',
                "error_rate": f'rate(order_errors_total{{app="{self.deployment_name}"}}[5m]) / rate(orders_processed_total{{app="{self.deployment_name}"}}[5m])',
                "throughput": f'rate(orders_processed_total{{app="{self.deployment_name}"}}[5m])'
            }
            
            results = {}
            all_within_limits = True
            
            for name, query in queries.items():
                try:
                    response = requests.get(
                        f"{prometheus_url}/api/v1/query",
                        params={"query": query},
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data["status"] == "success" and data["data"]["result"]:
                            value = float(data["data"]["result"][0]["value"][1])
                            results[name] = value
                            
                            # 检查是否在允许范围内
                            if name == "latency_p99" and value > 5000000:  # 5ms
                                all_within_limits = False
                            elif name == "error_rate" and value > 0.01:  # 1%
                                all_within_limits = False
                    
                except Exception as e:
                    LOG_WARN(f"Error querying metric {name}: {e}")
                    results[name] = None
            
            return {
                "name": "performance_metrics",
                "passed": all_within_limits,
                "details": {"metrics": results},
                "timestamp": time.time()
            }
            
        except Exception as e:
            LOG_WARN(f"Error checking performance metrics: {e}")
            return {
                "name": "performance_metrics",
                "passed": True,  # 不因监控系统问题导致部署失败
                "details": {"error": str(e)},
                "timestamp": time.time()
            }
    
    def rollback_if_needed(self, validation_passed: bool, 
                          validation_results: List[Dict]) -> bool:
        """如果需要则执行回滚"""
        if validation_passed:
            LOG_INFO("Deployment validation passed, no rollback needed")
            return False
        
        LOG_WARN("Deployment validation failed, initiating rollback")
        
        try:
            # 1. 检查是否有之前的版本可回滚
            if not self.previous_revision:
                LOG_ERROR("No previous revision recorded, cannot rollback")
                return False
            
            # 2. 执行回滚
            self._perform_rollback()
            
            # 3. 验证回滚是否成功
            rollback_success = self._verify_rollback()
            
            if rollback_success:
                LOG_INFO("Rollback completed successfully")
                
                # 发送回滚通知
                self._send_rollback_notification(validation_results)
            else:
                LOG_ERROR("Rollback failed")
                
            return rollback_success
            
        except Exception as e:
            LOG_ERROR(f"Error during rollback: {e}")
            return False
    
    def _perform_rollback(self):
        """执行回滚操作"""
        from kubernetes import client
        
        v1 = client.AppsV1Api()
        
        # 获取当前部署
        deployment = v1.read_namespaced_deployment(
            name=self.deployment_name,
            namespace=self.namespace
        )
        
        # 恢复到之前的镜像
        deployment.spec.template.spec.containers[0].image = self.previous_revision["image"]
        
        # 恢复到之前的副本数
        deployment.spec.replicas = self.previous_revision["replicas"]
        
        # 添加回滚注释
        if not deployment.metadata.annotations:
            deployment.metadata.annotations = {}
        
        deployment.metadata.annotations["rollback.reason"] = "deployment_validation_failed"
        deployment.metadata.annotations["rollback.timestamp"] = str(time.time())
        deployment.metadata.annotations["rollback.previous_version"] = self.previous_revision["image"]
        
        # 更新部署
        v1.replace_namespaced_deployment(
            name=self.deployment_name,
            namespace=self.namespace,
            body=deployment
        )
        
        LOG_INFO(f"Rolled back to previous image: {self.previous_revision['image']}")
    
    def _verify_rollback(self) -> bool:
        """验证回滚是否成功"""
        # 等待部署稳定
        time.sleep(10)
        
        # 检查部署状态
        deployment_status = self._check_deployment_status()
        
        if not deployment_status["passed"]:
            LOG_ERROR("Rollback deployment status check failed")
            return False
        
        # 检查Pod状态
        pod_status = self._check_pod_status()
        
        if not pod_status["passed"]:
            LOG_ERROR("Rollback pod status check failed")
            return False
        
        # 检查是否恢复到正确镜像
        deployment = self.k8s_client.read_namespaced_deployment(
            name=self.deployment_name,
            namespace=self.namespace
        )
        
        current_image = deployment.spec.template.spec.containers[0].image
        expected_image = self.previous_revision["image"]
        
        if current_image != expected_image:
            LOG_ERROR(f"Image mismatch after rollback: {current_image} != {expected_image}")
            return False
        
        LOG_INFO("Rollback verification passed")
        return True
    
    def _send_rollback_notification(self, validation_results: List[Dict]):
        """发送回滚通知"""
        try:
            failed_checks = [
                result for result in validation_results 
                if not result["passed"]
            ]
            
            notification_data = {
                "deployment": self.deployment_name,
                "namespace": self.namespace,
                "previous_version": self.previous_revision["image"],
                "rollback_time": time.time(),
                "failed_checks": failed_checks,
                "total_checks": len(validation_results)
            }
            
            requests.post(
                "https://monitoring.trading.com/api/rollback-notification",
                json=notification_data,
                timeout=10
            )
            
            LOG_INFO("Rollback notification sent")
            
        except Exception as e:
            LOG_WARN(f"Failed to send rollback notification: {e}")
```

## 5. 运维自动化与自愈

### 5.1 自动化运维工作流

```yaml
# automation/maintenance-workflows.yaml
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: trading-platform-maintenance
  namespace: argo
spec:
  entrypoint: maintenance-pipeline
  arguments:
    parameters:
      - name: maintenance-type
        value: "database-optimization"
      - name: environment
        value: "production"
      - name: scheduled-time
        value: "2024-01-15T02:00:00Z"
  
  templates:
    # 主维护管道
    - name: maintenance-pipeline
      steps:
        - - name: pre-maintenance-checks
            template: pre-checks
        
        - - name: execute-maintenance
            template: "{{workflow.parameters.maintenance-type}}"
            when: "{{steps.pre-maintenance-checks.outputs.result}} == true"
        
        - - name: post-maintenance-validation
            template: post-validation
            when: "{{steps.execute-maintenance.outputs.result}} == true"
        
        - - name: notify-completion
            template: send-notification
            arguments:
              parameters:
                - name: status
                  value: "{{steps.post-maintenance-validation.outputs.result}}"
    
    # 数据库优化维护
    - name: database-optimization
      inputs:
        parameters:
          - name: environment
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result
      
      container:
        image: postgres:15
        command: ["bash", "-c"]
        args:
          - |
            set -e
            
            echo "Starting database optimization for {{inputs.parameters.environment}}"
            
            # 1. 收集统计信息
            echo "Collecting database statistics..."
            psql -h postgres-master -U maintenance -c "ANALYZE;"
            
            # 2. 重新索引
            echo "Reindexing tables..."
            psql -h postgres-master -U maintenance -c "REINDEX DATABASE trading;"
            
            # 3. 清理膨胀
            echo "Cleaning up bloat..."
            psql -h postgres-master -U maintenance -c "VACUUM ANALYZE;"
            
            # 4. 更新查询计划器统计
            echo "Updating planner statistics..."
            psql -h postgres-master -U maintenance -c "SELECT pg_stat_statements_reset();"
            
            echo "true" > /tmp/result
            echo "Database optimization completed"
        
        env:
          - name: PGPASSWORD
            valueFrom:
              secretKeyRef:
                name: database-maintenance-secret
                key: password
    
    # 缓存预热工作流
    - name: cache-warmup
      inputs:
        parameters:
          - name: environment
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result
      
      steps:
        - - name: warmup-redis
            template: warmup-redis-cache
        
        - - name: warmup-local-cache
            template: warmup-local-cache
        
        - - name: verify-cache-warmup
            template: verify-cache-coverage
            arguments:
              parameters:
                - name: expected-hit-rate
                  value: "0.85"
    
    # 服务重启（滚动重启）
    - name: service-rolling-restart
      inputs:
        parameters:
          - name: service-name
          - name: batch-size
            value: "1"
          - name: wait-time
            value: "30"
      
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result
      
      container:
        image: bitnami/kubectl:latest
        command: ["bash", "-c"]
        args:
          - |
            set -e
            
            SERVICE={{inputs.parameters.service-name}}
            BATCH_SIZE={{inputs.parameters.batch-size}}
            WAIT_TIME={{inputs.parameters.wait-time}}
            
            echo "Starting rolling restart for $SERVICE"
            
            # 获取当前Pod列表
            PODS=$(kubectl get pods -l app=$SERVICE -o jsonpath='{.items[*].metadata.name}')
            POD_ARRAY=($PODS)
            TOTAL_PODS=${#POD_ARRAY[@]}
            
            echo "Total pods: $TOTAL_PODS"
            echo "Batch size: $BATCH_SIZE"
            
            # 分批重启
            for ((i=0; i<TOTAL_PODS; i+=BATCH_SIZE)); do
              echo "Processing batch $((i/BATCH_SIZE + 1))"
              
              for ((j=0; j<BATCH_SIZE && i+j<TOTAL_PODS; j++)); do
                POD=${POD_ARRAY[$i+$j]}
                echo "Restarting pod: $POD"
                
                # 优雅删除Pod（由Deployment控制器重新创建）
                kubectl delete pod $POD --wait=false
              done
              
              # 等待批次完成
              if [ $((i+BATCH_SIZE)) -lt $TOTAL_PODS ]; then
                echo "Waiting ${WAIT_TIME}s for batch to stabilize..."
                sleep $WAIT_TIME
                
                # 检查批次健康状态
                kubectl wait --for=condition=ready \
                  --timeout=60s \
                  pod -l app=$SERVICE
              fi
            done
            
            echo "Rolling restart completed"
            echo "true" > /tmp/result
    
    # 预维护检查
    - name: pre-checks
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result
      
      container:
        image: bitnami/kubectl:latest
        command: ["bash", "-c"]
        args:
          - |
            set -e
            
            echo "Running pre-maintenance checks..."
            
            # 1. 检查系统负载
            LOAD=$(kubectl top nodes | awk 'NR>1 {print $2}' | sort -nr | head -1)
            if [ "${LOAD%.*}" -gt 80 ]; then
              echo "ERROR: System load too high: $LOAD%"
              echo "false" > /tmp/result
              exit 1
            fi
            
            # 2. 检查服务状态
            SERVICES=("trading-engine" "risk-engine" "market-data")
            for SERVICE in "${SERVICES[@]}"; do
              READY=$(kubectl get deployment $SERVICE -o jsonpath='{.status.readyReplicas}')
              DESIRED=$(kubectl get deployment $SERVICE -o jsonpath='{.spec.replicas}')
              
              if [ "$READY" != "$DESIRED" ]; then
                echo "ERROR: Service $SERVICE not fully ready: $READY/$DESIRED"
                echo "false" > /tmp/result
                exit 1
              fi
            done
            
            # 3. 检查数据库连接
            if ! psql -h postgres-master -U healthcheck -c "SELECT 1;" > /dev/null 2>&1; then
              echo "ERROR: Database connection failed"
              echo "false" > /tmp/result
              exit 1
            fi
            
            # 4. 检查备份状态
            LAST_BACKUP=$(aws s3 ls s3://trading-backups/database/full/ | sort -r | head -1 | awk '{print $1}')
            BACKUP_AGE=$(( ($(date +%s) - $(date -d "$LAST_BACKUP" +%s)) / 3600 ))
            
            if [ "$BACKUP_AGE" -gt 24 ]; then
              echo "WARNING: Last backup is $BACKUP_AGE hours old"
            fi
            
            echo "Pre-maintenance checks passed"
            echo "true" > /tmp/result
    
    # 维护后验证
    - name: post-validation
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result
      
      container:
        image: bitnami/kubectl:latest
        command: ["bash", "-c"]
        args:
          - |
            set -e
            
            echo "Running post-maintenance validation..."
            
            # 1. 验证服务健康
            SERVICES=("trading-engine" "risk-engine" "market-data")
            for SERVICE in "${SERVICES[@]}"; do
              echo "Checking service: $SERVICE"
              
              # 等待服务就绪
              kubectl wait --for=condition=available \
                --timeout=300s \
                deployment/$SERVICE
              
              # 检查端点健康
              ENDPOINT="http://$SERVICE.trading-production.svc.cluster.local:8080/health"
              if ! curl -s --max-time 10 "$ENDPOINT" | grep -q '"status":"UP"'; then
                echo "ERROR: Service $SERVICE health check failed"
                echo "false" > /tmp/result
                exit 1
              fi
            done
            
            # 2. 验证数据库性能
            echo "Checking database performance..."
            QUERY_TIME=$(psql -h postgres-master -U validator -t -c "\timing on; SELECT COUNT(*) FROM orders LIMIT 1;" | tail -1 | awk '{print $2}')
            
            if (( $(echo "$QUERY_TIME > 1000" | bc -l) )); then
              echo "WARNING: Database query time high: ${QUERY_TIME}ms"
            fi
            
            # 3. 验证业务功能
            echo "Checking business functionality..."
            
            # 尝试下一笔测试订单
            TEST_RESULT=$(curl -s -X POST \
              -H "Content-Type: application/json" \
              -d '{"instrument_id":"rb2405", "price":3800, "volume":1, "direction":"BUY"}' \
              http://trading-engine.trading-production.svc.cluster.local:8080/api/test-order)
            
            if ! echo "$TEST_RESULT" | grep -q '"success":true'; then
              echo "ERROR: Test order failed: $TEST_RESULT"
              echo "false" > /tmp/result
              exit 1
            fi
            
            echo "Post-maintenance validation passed"
            echo "true" > /tmp/result
```

## 6. 容量规划与成本优化

### 6.1 容量规划模型

```python
# Python 容量规划与预测模型
class CapacityPlanner:
    """容量规划与资源预测模型"""
    
    def __init__(self, historical_data: pd.DataFrame):
        self.historical_data = historical_data
        self.models = {}
        self.seasonal_patterns = self._detect_seasonal_patterns()
        
    def _detect_seasonal_patterns(self) -> Dict:
        """检测季节性模式"""
        patterns = {
            "daily": self._analyze_daily_pattern(),
            "weekly": self._analyze_weekly_pattern(),
            "monthly": self._analyze_monthly_pattern(),
            "intraday": self._analyze_intraday_pattern()
        }
        return patterns
    
    def predict_resource_demand(self, 
                              forecast_days: int = 30,
                              confidence_level: float = 0.95) -> Dict:
        """预测资源需求"""
        predictions = {
            "cpu": self._predict_cpu_demand(forecast_days, confidence_level),
            "memory": self._predict_memory_demand(forecast_days, confidence_level),
            "storage": self._predict_storage_demand(forecast_days, confidence_level),
            "network": self._predict_network_demand(forecast_days, confidence_level),
            "cost": self._predict_cost(forecast_days, confidence_level)
        }
        
        # 添加容量建议
        predictions["recommendations"] = self._generate_recommendations(predictions)
        
        return predictions
    
    def _predict_cpu_demand(self, forecast_days: int, 
                           confidence_level: float) -> Dict:
        """预测CPU需求"""
        # 使用时间序列模型（SARIMA）
        from statsmodels.tsa.statespace.sarimax import SARIMAX
        
        # 准备数据
        cpu_data = self.historical_data['cpu_usage'].resample('H').mean()
        
        # 拟合模型（考虑季节性）
        model = SARIMAX(
            cpu_data,
            order=(1, 1, 1),
            seasonal_order=(1, 1, 1, 24),  # 24小时季节性
            enforce_stationarity=False,
            enforce_invertibility=False
        )
        
        fitted_model = model.fit(disp=False)
        
        # 预测
        forecast = fitted_model.get_forecast(steps=forecast_days * 24)
        forecast_mean = forecast.predicted_mean
        forecast_conf = forecast.conf_int(alpha=1 - confidence_level)
        
        # 计算峰值需求（考虑P99）
        peak_hourly = forecast_mean.rolling(24).max().max()
        
        return {
            "daily_average": forecast_mean.resample('D').mean().tolist(),
            "peak_hourly": float(peak_hourly),
            "confidence_interval": {
                "lower": forecast_conf.iloc[:, 0].tolist(),
                "upper": forecast_conf.iloc[:, 1].tolist()
            },
            "required_cores": self._calculate_required_cores(float(peak_hourly))
        }
    
    def _calculate_required_cores(self, peak_cpu_percent: float) -> int:
        """计算所需CPU核心数"""
        # 考虑超线程和预留
        current_cores = 64  # 假设当前有64核
        current_usage = current_cores * (peak_cpu_percent / 100)
        
        # 目标利用率（70%）
        target_utilization = 0.7
        
        # 安全余量（20%）
        safety_margin = 1.2
        
        required_cores = (current_usage / target_utilization) * safety_margin
        
        # 向上取整到最接近的整数
        return math.ceil(required_cores)
    
    def _predict_memory_demand(self, forecast_days: int,
                              confidence_level: float) -> Dict:
        """预测内存需求"""
        # 内存需求通常更稳定，使用线性回归+季节性调整
        memory_data = self.historical_data['memory_usage'].resample('D').max()
        
        # 添加趋势和季节性特征
        memory_df = pd.DataFrame({
            'memory': memory_data.values,
            'trend': np.arange(len(memory_data)),
            'day_of_week': memory_data.index.dayofweek,
            'month': memory_data.index.month
        })
        
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import OneHotEncoder
        
        # 编码分类变量
        encoder = OneHotEncoder(sparse_output=False)
        encoded_features = encoder.fit_transform(memory_df[['day_of_week', 'month']])
        
        # 组合特征
        X = np.column_stack([memory_df['trend'].values, encoded_features])
        y = memory_df['memory'].values
        
        # 训练模型
        model = LinearRegression()
        model.fit(X, y)
        
        # 预测未来
        future_dates = pd.date_range(
            start=memory_data.index[-1] + pd.Timedelta(days=1),
            periods=forecast_days,
            freq='D'
        )
        
        future_df = pd.DataFrame({
            'trend': np.arange(len(memory_data), len(memory_data) + forecast_days),
            'day_of_week': future_dates.dayofweek,
            'month': future_dates.month
        })
        
        future_encoded = encoder.transform(future_df[['day_of_week', 'month']])
        future_X = np.column_stack([future_df['trend'].values, future_encoded])
        
        predictions = model.predict(future_X)
        
        # 计算置信区间（使用残差）
        residuals = y - model.predict(X)
        std_residuals = np.std(residuals)
        z_score = stats.norm.ppf((1 + confidence_level) / 2)
        
        margin_error = z_score * std_residuals
        
        return {
            "daily_max": predictions.tolist(),
            "current_peak": float(memory_data.max()),
            "growth_rate": self._calculate_growth_rate(memory_data),
            "confidence_interval": {
                "lower": (predictions - margin_error).tolist(),
                "upper": (predictions + margin_error).tolist()
            },
            "required_memory_gb": self._calculate_required_memory(float(predictions.max()))
        }
    
    def _calculate_required_memory(self, peak_memory_gb: float) -> int:
        """计算所需内存（GB）"""
        # 考虑内存超配和预留
        overcommit_ratio = 1.1  # 10%超配
        reserved_memory_gb = 16  # 系统预留内存
        
        required = (peak_memory_gb * overcommit_ratio) + reserved_memory_gb
        
        # 向上取整到最接近的整数
        return math.ceil(required)
    
    def _generate_recommendations(self, predictions: Dict) -> List[Dict]:
        """生成容量规划建议"""
        recommendations = []
        
        # CPU建议
        current_cpu = 64  # 假设当前64核
        required_cpu = predictions["cpu"]["required_cores"]
        
        if required_cpu > current_cpu * 1.2:  # 超过20%增长
            recommendations.append({
                "type": "SCALE_UP",
                "resource": "CPU",
                "current": current_cpu,
                "required": required_cpu,
                "increase_pct": ((required_cpu - current_cpu) / current_cpu) * 100,
                "priority": "HIGH",
                "timeline": "1 month",
                "estimated_cost": self._estimate_cpu_cost(required_cpu - current_cpu)
            })
        elif required_cpu < current_cpu * 0.8:  # 低于80%使用率
            recommendations.append({
                "type": "SCALE_DOWN",
                "resource": "CPU",
                "current": current_cpu,
                "recommended": math.ceil(current_cpu * 0.8),
                "savings_pct": ((current_cpu - math.ceil(current_cpu * 0.8)) / current_cpu) * 100,
                "priority": "MEDIUM",
                "timeline": "Next maintenance window",
                "estimated_savings": self._estimate_cpu_savings(current_cpu - math.ceil(current_cpu * 0.8))
            })
        
        # 内存建议
        current_memory_gb = 512  # 假设当前512GB
        required_memory_gb = predictions["memory"]["required_memory_gb"]
        
        if required_memory_gb > current_memory_gb * 1.15:  # 超过15%增长
            recommendations.append({
                "type": "SCALE_UP",
                "resource": "Memory",
                "current": current_memory_gb,
                "required": required_memory_gb,
                "increase_pct": ((required_memory_gb - current_memory_gb) / current_memory_gb) * 100,
                "priority": "HIGH",
                "timeline": "2 weeks",
                "estimated_cost": self._estimate_memory_cost(required_memory_gb - current_memory_gb)
            })
        
        # 存储建议
        storage_growth = predictions["storage"].get("growth_rate_gb_per_day", 0)
        if storage_growth > 10:  # 每天增长超过10GB
            recommendations.append({
                "type": "STORAGE_EXPANSION",
                "resource": "Storage",
                "current_growth_rate": storage_growth,
                "projected_90day_growth": storage_growth * 90,
                "priority": "MEDIUM",
                "timeline": "Next quarter",
                "recommended_action": "Add 1TB SSD storage"
            })
        
        # 成本优化建议
        current_monthly_cost = 50000  # 假设当前每月$50,000
        projected_cost = predictions["cost"].get("monthly", current_monthly_cost)
        
        if projected_cost > current_monthly_cost * 1.1:  # 成本增长超过10%
            recommendations.append({
                "type": "COST_OPTIMIZATION",
                "current_monthly": current_monthly_cost,
                "projected_monthly": projected_cost,
                "increase_pct": ((projected_cost - current_monthly_cost) / current_monthly_cost) * 100,
                "priority": "HIGH",
                "recommended_actions": [
                    "Review reserved instance usage",
                    "Optimize storage classes",
                    "Implement auto-scaling policies"
                ]
            })
        
        return recommendations
```

### 6.2 成本优化策略

```yaml
# cost-optimization/cost-management.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-optimization-config
  namespace: kube-system
data:
  # 资源优化配置
  resource-optimization: |
    {
      "rightsizing": {
        "enabled": true,
        "schedule": "weekly",
        "target_utilization": {
          "cpu": 0.7,
          "memory": 0.8,
          "storage": 0.85
        },
        "min_savings_threshold": 0.1  # 至少节省10%才执行
      },
      
      "autoscaling": {
        "horizontal": {
          "min_replicas": 2,
          "max_replicas": 10,
          "target_cpu_utilization": 70,
          "target_memory_utilization": 80,
          "scale_down_delay": 300
        },
        "vertical": {
          "enabled": true,
          "update_mode": "Auto",
          "min_allowed": {
            "cpu": "100m",
            "memory": "128Mi"
          },
          "max_allowed": {
            "cpu": "8",
            "memory": "16Gi"
          }
        }
      },
      
      "scheduling_optimization": {
        "bin_packing": {
          "enabled": true,
          "strategy": "MostAllocated"
        },
        "node_affinity": {
          "prefer_low_cost_nodes": true
        },
        "spot_instances": {
          "enabled": true,
          "max_price_percentage": 30,
          "interruption_handling": "checkpoint_and_restart"
        }
      }
    }
  
  # 存储优化配置
  storage-optimization: |
    {
      "tiering": {
        "hot_storage": {
          "class": "gp3",
          "retention_days": 7,
          "auto_tiering": true
        },
        "warm_storage": {
          "class": "standard",
          "retention_days": 30,
          "auto_tiering": true
        },
        "cold_storage": {
          "class": "glacier",
          "retention_days": 365,
          "auto_tiering": true
        }
      },
      
      "compression": {
        "enabled": true,
        "algorithm": "zstd",
        "level": 3,
        "min_size_mb": 1
      },
      
      "deduplication": {
        "enabled": true,
        "block_size": "4KB",
        "min_savings_threshold": 0.05
      },
      
      "lifecycle_policies": {
        "logs": {
          "delete_after_days": 30,
          "archive_to_cold_after_days": 7
        },
        "backups": {
          "delete_after_days": 90,
          "archive_to_cold_after_days": 30
        },
        "metrics": {
          "downsample_after_days": 7,
          "delete_after_days": 365
        }
      }
    }
  
  # 网络优化配置
  network-optimization: |
    {
      "traffic_optimization": {
        "compression": {
          "enabled": true,
          "min_size_kb": 1,
          "algorithms": ["gzip", "brotli"]
        },
        "caching": {
          "enabled": true,
          "ttl": 300,
          "max_size_mb": 100
        }
      },
      
      "cdn_optimization": {
        "enabled": true,
        "providers": ["cloudfront", "akamai"],
        "static_content": {
          "cache_ttl": 86400,
          "compression": true
        },
        "dynamic_content": {
          "cache_ttl": 60,
          "edge_compute": true
        }
      },
      
      "data_transfer": {
        "intra_region": {
          "prefer_private_ips": true,
          "use_vpc_endpoints": true
        },
        "cross_region": {
          "compression": true,
          "batch_size_mb": 10,
          "schedule": "off_peak"
        }
      }
    }

---
# 成本监控与告警
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cost-monitoring-rules
  namespace: monitoring
spec:
  groups:
    - name: cost-optimization
      rules:
        # 成本超支告警
        - alert: CostOverrun
          expr: |
            aws_cost_current_month > aws_cost_budget_month * 1.1
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "AWS成本超过预算10%"
            description: |
              当前月AWS成本为{{ $value }}元，超过预算{{ $labels.budget }}元的10%。
              请检查资源使用情况。
        
        # 资源浪费检测
        - alert: ResourceWaste
          expr: |
            (
              kube_pod_container_resource_requests{resource="cpu"} * 0.3
              > kube_pod_container_resource_usage{resource="cpu"}
            ) or (
              kube_pod_container_resource_requests{resource="memory"} * 0.3
              > kube_pod_container_resource_usage{resource="memory"}
            )
          for: 24h
          labels:
            severity: info
          annotations:
            summary: "资源请求远高于实际使用"
            description: |
              Pod {{ $labels.pod }} 的{{ $labels.resource }}请求为{{ $labels.value }}，
              但实际使用仅为{{ $labels.usage }}，利用率低于30%。
              建议调整资源请求。
        
        # 存储成本优化
        - alert: StorageCostOptimization
          expr: |
            aws_s3_storage_size_bytes{storage_class="STANDARD"} 
            > aws_s3_storage_size_bytes{storage_class="STANDARD_IA"} * 2
          for: 7d
          labels:
            severity: info
          annotations:
            summary: "标准存储使用量是低频存储的2倍以上"
            description: |
              标准存储使用量为{{ $value }}字节，而低频存储仅为{{ $labels.standard_ia_size }}字节。
              考虑将不常访问的数据迁移到低频存储。
        
        # 空闲资源检测
        - alert: IdleResources
          expr: |
            (
              avg_over_time(kube_node_status_capacity_cpu_cores[7d])
              - avg_over_time(kube_node_status_allocatable_cpu_cores[7d])
            ) > 4
          for: 7d
          labels:
            severity: warning
          annotations:
            summary: "节点有大量未分配CPU资源"
            description: |
              节点 {{ $labels.node }} 在过去7天平均有{{ $value }}个CPU核心未分配。
              考虑缩小节点规模或迁移工作负载。
```

## 7. 总结与演进

### 7.1 部署与灾备成熟度模型

| 成熟度等级 | 部署能力 | 灾备能力 | 自动化水平 | 恢复目标 |
|------------|----------|----------|------------|----------|
| **初始级** | 手动部署 | 基本备份 | 无自动化 | RTO: 24h, RPO: 24h |
| **可重复级** | 脚本化部署 | 定期备份 | 基础脚本 | RTO: 8h, RPO: 4h |
| **已定义级** | CI/CD流水线 | 跨区域备份 | 部分自动化 | RTO: 2h, RPO: 1h |
| **已管理级** | 蓝绿部署 | 热备站点 | 高度自动化 | RTO: 30m, RPO: 5m |
| **优化级** | 零停机部署 | 多活架构 | 全自动化 | RTO: 5m, RPO: 1s |

### 7.2 关键成功指标

| 指标类别 | 指标名称 | 目标值 | 测量频率 |
|----------|----------|--------|----------|
| 部署效率 | 部署频率 | 每天多次 | 实时 |
|  | 部署成功率 | > 99.9% | 每次部署 |
|  | 部署平均时间 | < 15分钟 | 每次部署 |
|  | 回滚成功率 | 100% | 每次回滚 |
| 系统可用性 | 服务可用性 | 99.99% | 每分钟 |
|  | 灾备切换成功率 | 100% | 每次演练 |
|  | 数据零丢失率 | 100% | 每次切换 |
| 运维效率 | 变更成功率 | > 99.5% | 每次变更 |
|  | 故障恢复时间 | < 5分钟 | 每次故障 |
|  | 人工干预比例 | < 10% | 每月 |
| 成本效率 | 资源利用率 | > 70% | 每小时 |
|  | 成本预算符合率 | 100% | 每月 |
|  | 自动化节约率 | > 40% | 每季度 |

### 7.3 演进路线图

**阶段1：基础自动化（1-3个月）**
- 实现CI/CD基础流水线
- 建立容器化部署能力
- 完成基础监控告警

**阶段2：高级部署能力（3-6个月）**
- 实现蓝绿/金丝雀部署
- 建立灾备基础架构
- 完善自动化测试

**阶段3：全自动化运维（6-12个月）**
- 实现零停机部署
- 建立多活灾备架构
- 部署自愈与优化系统

**阶段4：智能运维（12-24个月）**
- AI驱动的容量预测
- 智能故障预测与预防
- 全栈自动化优化

### 7.4 团队与技能要求

**平台工程团队（4-5人）：**
- 平台架构师：整体架构设计、技术选型
- DevOps工程师：CI/CD流水线、部署工具
- SRE工程师：可靠性工程、容量规划
- 安全工程师：安全合规、密钥管理

**应用开发团队（3-4人）：**
- 后端开发：微服务开发、API设计
- 前端开发：监控界面、管理控制台
- 测试工程师：自动化测试、性能测试

**基础设施团队（2-3人）：**
- 网络工程师：网络架构、安全策略
- 存储工程师：存储优化、备份恢复
- 云架构师：云资源管理、成本优化

**运维支持团队（1-2人）：**
- 运维工程师：日常运维、故障处理
- 技术支持：用户支持、文档管理

通过本部署、灾备与持续集成方案的设计与实施，平台将建立起从代码提交到生产部署的全自动化流水线，实现高频交易系统的高可用、高可靠运行，同时满足中国金融市场的严格监管要求。